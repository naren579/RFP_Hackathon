If you are consolidating older and smaller appliances with new models or larger capacity appliances, consider cloning the appliance node (or use appliance node cloning and the decommission procedure if you aren’t doing a one-to-one replacement).

For example, you might add two new, larger capacity Storage Nodes to replace three older Storage Nodes. You would first use the expansion procedure to add the two new, larger Storage Nodes, and then use the decommission procedure to remove the three old, smaller capacity Storage Nodes. By adding new capacity before removing existing Storage Nodes, you ensure a more balanced distribution of

1442

data across the StorageGRID system. You also reduce the possibility that an existing Storage Node might be pushed beyond the storage watermark level. Decommission multiple Storage Nodes You can consolidate Storage Nodes to reduce the Storage Node count for a site or deployment while increasing storage capacity.

When you consolidate Storage Nodes, you expand the StorageGRID system by adding new, larger capacity Storage Nodes and then decommission the old, smaller capacity Storage Nodes. During the decommission procedure, objects are migrated from the old Storage Nodes to the new Storage Nodes.

If you are consolidating older and smaller appliances with new models or larger capacity appliances, consider cloning the appliance node (or use appliance node cloning and the decommission procedure if you aren’t doing a one-to-one replacement).

For example, you might add two new, larger capacity Storage Nodes to replace three older Storage Nodes. You would first use the expansion procedure to add the two new, larger Storage Nodes, and then use the decommission procedure to remove the three old, smaller capacity Storage Nodes. By adding new capacity before removing existing Storage Nodes, you ensure a more balanced distribution of

1442

data across the StorageGRID system. You also reduce the possibility that an existing Storage Node might be pushed beyond the storage watermark level. Decommission multiple Storage Nodes

If you need to remove more than one Storage Node, you can decommission them either sequentially or in parallel.

Use caution when you decommission Storage Nodes in a grid containing software-based metadata-only nodes. If you decommission all nodes configured to store both objects and metadata, the ability to store objects is removed from the grid. See Types of Storage Nodes for more information about metadata-only Storage Nodes.

If you decommission Storage Nodes sequentially, you must wait for the first Storage Node to complete decommissioning before starting to decommission the next Storage Node.

If you decommission Storage Nodes in parallel, the Storage Nodes simultaneously process decommission tasks for all Storage Nodes being decommissioned. This can result in a situation where all permanent copies of a file are marked as "read‐only," temporarily disabling deletion in grids where this functionality is enabled. Check data repair jobs

Before decommissioning a grid node, you must confirm that no data repair jobs are active. If any repairs have failed, you must restart them and allow them to complete before performing the decommission procedure.

About this task

If you need to decommission a disconnected Storage Node, you will also complete these steps after the decommission procedure completes to ensure the data repair job has completed successfully. You must ensure that any erasure-coded fragments that were on the removed node have been restored successfully.

These steps only apply to systems that have erasure-coded objects. Steps

1. Log in to the primary Admin Node:

a. Enter the following command: ssh admin@grid_node_IP

b. Enter the password listed in the Passwords.txt file.

c. Enter the following command to switch to root: su -

d. Enter the password listed in the Passwords.txt file.

When you are logged in as root, the prompt changes from $ to #.

2. Check for running repairs: repair-data show-ec-repair-status

If you have never run a data repair job, the output is No job found. You don’t need to restart any repair jobs.

If the data repair job was run previously or is running currently, the output lists information for the repair. Each repair has a unique repair ID.

1443

Optionally, you can use the Grid Manager to monitor restoration processes in progress and display a restoration history. See Restore object data using Grid Manager.

3. If the State for all repairs is Completed, you don’t need to restart any repair jobs.

4. If the State for any repair is Stopped, you must restart that repair. a. Obtain the repair ID for the failed repair from the output.

b. Run the repair-data start-ec-node-repair command.

Use the --repair-id option to specify the Repair ID. For example, if you want to retry a repair with repair ID 949292, run this command: repair-data start-ec-node-repair --repair-id 949292