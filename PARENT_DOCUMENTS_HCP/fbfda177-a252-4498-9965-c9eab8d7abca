It is not WORM It may or may not have a cryptographic hash value

It is not subject to retention It cannot have custom metadata

It is not indexed It is not replicated

Note: If you observe multiple stale NFS file handle errors on an NFS client, you might resolve the issue by disabling client-side caching. To do this, add the mount option lookupcac he=none to the command. The complete set of options will be:

o tcp,vers=3,timeo=600,hard,intr,lookupcache=none

Setting this option will enhance the consistency of client access and may assist in mitigating the issue. However, it's important to note that it may also result in a significant degradation of performance.

Failed NFS write operations An NFS write operation is considered to have failed if the target node failed while the object was open for write. Also, in some circumstances, a write operation is considered to have failed if another node or other hardware failed while the object was open for write.

An NFS write operation is not considered to have failed if the TCP connection broke. This is because HCP doesnâ€™t see the failure. In this case, lazy close applies, and the object is

considered complete.

Objects left by failed NFS write operations:

May have none, some, or all of their data

https://docs.hitachivantara.com/internal/api/webapp/print/72cda581-a515-4975-93dd-f591140b46a3

801/907

6/25/24, 11:34 AM

Content Platform System Management Help

If partially written, may or may not have a cryptographic hash value If the failure was on the HCP side, remain open and:

Are not WORM Cannot have annotations Are not indexed

Are not replicated

If the failure was on the client side, are WORM after the lazy close

If a write operation fails, delete the object and try the write operation again.

Note: If the object is WORM, any retention setting applies. In this case, you may not be able to delete the object.

Storing zero-sized files with NFS

When you store a zero-sized file with NFS, the resulting object has no data. After lazy close occurs, the object becomes WORM and is treated like any other object in the namespace.

Out-of-order writes with NFS

NFS can write the data for an object out of order. If HCP receives an out-of-order write for a large file (200,000 bytes or larger), it discards the cryptographic hash value it already

calculated. The object then has no hash value until either of these occurs:

HCP returns to the object at a later time and calculates the hash value for it. A user or application opens or downloads the hash.txt metafile for the object, which causes HCP to calculate the hash value. However, because HCP calculates this value

asynchronously, the value may not be immediately available. This is particularly true for large objects. NFS reads of large objects

While HCP is reading very large objects (thousands of megabytes or more) through NFS, system performance decreases.

Walking large directory trees

HCP occasionally reuses inode numbers. Normally, this has no impact. However, it can affect programs that walk the directory tree, like the Unix du command. If you run such a program against a very large directory tree, it may not go down certain subdirectory paths.

One way to prevent this problem is to work on directory segments, instead of the entire directory tree. For example, when you use the du command you can run the command against smaller segments of the directory hierarchy; then add the returned values together to get the total. NFS delete operations

While an object is open for write through NFS on a given node, it cannot be deleted through NFS on other nodes.

NFS mounts on a failed node

If an HCP node fails, NFS mounts that target the failed node lose their connections to the namespace. To recover from a node failure, unmount the namespace at the current mount point. Then take one of these actions:

Mount the namespace on a different node. You can do this by specifying either the domain name of the HCP system or a different node IP address in the mount command. If